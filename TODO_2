Absolutely‚Äîhere‚Äôs your **updated, single TODO / roadmap** reflecting what we actually finished today (Windows-safe normalization, logging/progress knobs, dedupe script + PR), plus exactly what‚Äôs next with the **Git flow and run commands** you asked for.

---

# PhishBlocker ‚Äî Status Snapshot (latest)

## What‚Äôs done ‚úÖ

### App / APIs / Infra

* FastAPI app with `create_app()` factory, async `lifespan`, Request-ID middleware.
* Routers

  * **pages (HTML):** `/`, `POST /scan`, `/history`, `/scan/{id}/view` (end-to-end form ‚Üí pipeline ‚Üí DB ‚Üí detail view).
  * **scans (JSON/exports):** `POST /api/scan`, `GET /api/scans`, `GET /api/scan/{id}`, `GET /export/csv` (Excel formula hardening).
  * **debug:** `/health`, `/_debug/scan_count`, `/_debug/cwd`.
* Central logging & config: env-driven log level, Request-ID injected into all logs (incl. `uvicorn.*`), structured startup logs.
* App config (`app/core/config.py`): `ENV`, logging toggles, `MODEL_PATH`, `MODEL_VERSION`, `MODEL_METRICS_PATH`.
* Runtime model loading: `ModelService` instantiated once; graceful fallback to heuristics if artifact missing; attached to `app.state.model`.
* Database: SQLAlchemy `Email` & `Scan` models (FKs, indexes). `Scan` persists `label`, `confidence`, human-readable `reasons`, structured `details` (tokens/weights), full `probs`, `header_flags`, `model_version`, `created_at`.
* Dev DB bootstraps via `Base.metadata.create_all()`; prod path assumes Alembic.
* CSV export with label/date filters and safe output.

### Services

* `parser.py`: robust RFC-822 parsing (headers, bodies, URLs, auth results, IPs, tolerant HTML‚Üítext).
* `heuristics.py`: lightweight text flags; `(label, confidence, reasons)` fallback.
* `scan_pipeline.py`: persists `Email`, runs heuristics and/or ML, writes full `Scan` row including `details` and `probs`.

### Model service

* `app/services/model.py` with:

  * `predict_with_explanations(text, meta) -> (label, prob, reasons[])`
  * `predict_proba_map(text) -> {class: prob, ...}`
  * top-token explanations from LR coefficients; `.version` for audit.

### Trainer & tooling

* `scripts/train_baseline.py` (GPU/CPU-friendly TF-IDF + Torch LR): resilient IO, label normalization, metrics JSON, PR/ROC/CM plots, optional temperature scaling, threshold tuning helpers.
* Predictor script exists; artifacts and datasets are **gitignored** and documented.

### Datasets & ETL (new today)

* **PowerShell downloaders added** (`scripts/datasets/fetch_spamassassin.ps1`, `fetch_nazario.ps1`, `fetch_enron.ps1`) ‚Äî idempotent, resume-friendly.
* **Normalization script implemented & hardened**: `scripts/datasets/normalize_eml.py`

  * Windows-safe traversal of Enron (uses `os.walk` with `\\?\` extended paths; handles trailing-dot filenames).
  * Bytes/str-safe email extraction; HTML fallbacks; minimal cleaner.
  * **Debug & telemetry** flags: `--debug`, `--debug-sample`, `--ctype-sample`, `--max-candidates-per-source`, `--size-max-bytes`, `--progress-every`.
* **Exact dedupe implemented**: `scripts/datasets/dedupe_texts.py`

  * SHA-256 of normalized text (casefold + whitespace collapse); keep first per `(text,label)`.
  * Logs totals and by-label counts.
* **Active branch:** `feat/data-dedupe` (PR opened).

### Local smoke

* App runs on Windows with `uvicorn` (8080); `/`, `/history`, `/api/*` verified.
* Branch hygiene in place; committing in small steps.

---

## In progress üöß

* **Normalization full run** on Enron (now works; very large). Using new knobs to verify on samples, then scale.
* **Dedupe** to produce `mix_dedup.csv` after normalization outputs are ready.

---

## Next up (M2 ‚Äî Data & thresholds) üéØ

Below are bite-sized tasks with **branch names**, **git commands**, and **run commands**.

### 1) (If not merged) Land normalization fixes

**Branch:** `feat/data-normalize` (use if you want a separate PR for normalize changes)

```powershell
git switch -c feat/data-normalize
git add scripts/datasets/normalize_eml.py
git commit -m "feat(datasets): Windows-safe Enron traversal + debug/progress flags"
git push -u origin feat/data-normalize
gh pr create --base main --head feat/data-normalize `
  --title "Datasets: normalization (Windows-safe Enron + progress/limits)" `
  --body  "Use os.walk + \\?\\; add debug/progress/limits; robust extraction."
```

**Quick sample run (fast verification):**

```powershell
python scripts/datasets/normalize_eml.py `
  --spamassassin-root data/raw/spamassassin `
  --nazario-root      data/raw/nazario `
  --enron-maildir     data/raw/enron/maildir `
  --min-chars 5 `
  --out-dir           data/processed `
  --debug --ctype-sample 0 --debug-sample 3 `
  --max-candidates-per-source 5000 `
  --size-max-bytes 2000000 `
  --progress-every 2000
```

**Full run (later, no cap):**

```powershell
python scripts/datasets/normalize_eml.py `
  --spamassassin-root data/raw/spamassassin `
  --nazario-root      data/raw/nazario `
  --enron-maildir     data/raw/enron/maildir `
  --min-chars 20 `
  --out-dir           data/processed `
  --progress-every 20000
```

---

### 2) De-duplication (exact)

**Branch:** `feat/data-dedupe` (already created & pushed)

**Run after normalize CSVs exist:**

```powershell
python scripts/datasets/dedupe_texts.py `
  --inputs data/processed/spamassassin.csv data/processed/nazario.csv data/processed/enron.csv `
  --out    data/processed/mix_dedup.csv
```

**If you tweak the script, commit small:**

```powershell
git add scripts/datasets/dedupe_texts.py
git commit -m "chore(dedupe): tweak logging & edge-cases"
git push
# PR already exists; push updates to same branch
```

---

### 3) Balancing to target counts (‚â•100k/class)

**Goal:** produce a single balanced dataset with **100k per class** minimum (`safe`, `spam`, `phishing`).

**Branch:** `feat/data-balance-100k`

**What to implement:** `scripts/datasets/balance_split.py`

* Load normalized + deduped CSVs.
* Count available per class.
* Strategy:

  * **safe:** downsample Enron to target.
  * **spam:** all SpamAssassin spam sets; if short, document shortfall (allow param `--target-per-class`, warn when unmet).
  * **phishing:** Nazario is small; still produce max feasible balanced set and warn.
* Output:

  * `data/processed/mix_balanced.csv`, `mix_balanced.parquet`
  * Deterministic `train/val` (e.g., 80/20) using seeded split.

**Git flow:**

```powershell
git switch -c feat/data-balance-100k
git add scripts/datasets/balance_split.py
git commit -m "feat(datasets): balance+split targeting 100k/class; warn when sources underfill"
git push -u origin feat/data-balance-100k
gh pr create --base main --head feat/data-balance-100k `
  --title "Datasets: balance + split (target 100k/class)" `
  --body  "Downsample safe; combine spam; parameterized target with warnings; outputs CSV+Parquet and train/val split."
```

---

### 4) Metrics & thresholds loader at runtime

**Branch:** `feat/ml-threshold-runtime`

**Implement:**

* `app/services/thresholds.py`

  * read `settings.MODEL_METRICS_PATH`
  * `get_thresholds()` and `decide_label(probs, thresholds)` (e.g., `œÑ_phishing`, `œÑ_spam`; else safe).
* Update `scan_pipeline.py`

  * use threshold policy when model exists,
  * persist `thresholds_used` + `final_decision_source` in `Scan.details`.

**Git flow:**

```powershell
git switch -c feat/ml-threshold-runtime
git add app/services/thresholds.py
git commit -m "feat(ml): load thresholds from metrics.json and provide decision policy"
git add app/services/scan_pipeline.py
git commit -m "chore(pipeline): apply threshold policy; persist thresholds_used and decision_source"
git push -u origin feat/ml-threshold-runtime
gh pr create --base main --head feat/ml-threshold-runtime `
  --title "ML: runtime thresholds + decision policy" `
  --body  "Load œÑ from metrics; apply class thresholds; store in Scan.details."
```

---

### 5) UI/UX: surface operating point

**Branch:** `feat/ui-threshold-indicator`

**Implement on `/scan/{id}/view`:**

* Final label & confidence
* Threshold used for that class (e.g., ‚ÄúœÑ_phishing = 0.72‚Äù)
* Top tokens (already in `Scan.details`)

**Git flow:**

```powershell
git switch -c feat/ui-threshold-indicator
git add app/api/routes/pages.py app/templates/<updated>.html
git commit -m "feat(ui): show threshold policy, class confidence, and top tokens on scan detail"
git push -u origin feat/ui-threshold-indicator
gh pr create --base main --head feat/ui-threshold-indicator `
  --title "UI: threshold indicator on scan detail" `
  --body  "Expose œÑ per class, final confidence, and token explanations."
```

---

### 6) Filtering & feedback (kickoff)

**Branch:** `feat/history-filters-feedback`

**Implement:**

* Query params on `/history` and `/api/scans` (label/sender/date).
* Basic ‚ÄúCorrect label to ‚Ä¶‚Äù on detail page ‚Üí persist minimal `Correction` model (schema TBD).

**Git flow:**

```powershell
git switch -c feat/history-filters-feedback
git add app/api/routes/scans.py app/api/routes/pages.py app/models/corrections.py
git commit -m "feat(history): filters for label/sender/date and basic feedback capture"
git push -u origin feat/history-filters-feedback
gh pr create --base main --head feat/history-filters-feedback `
  --title "History: filters + feedback capture (kickoff)" `
  --body  "Adds query filters and a simple correction model for future retraining."
```

---

## Stretch (soon after M2)

* CI: pre-commit (black/ruff/isort), pytest seeds (parser & API), GitHub Actions (lint+test), Dockerfile + Alembic on start.
* Data retention: TTL job to redact `Email.body_text/html_raw` after N days; keep `Scan` metadata.
* Near-duplicate removal: MinHash/SimHash optional module when phishing sources scale.

---

## Running order (recommended)

1. `feat/scripts-refactor`
2. `feat/data-downloaders`
3. `feat/data-normalize` (‚úÖ implemented; finalize full run)
4. `feat/data-dedupe` (‚úÖ script + PR; run after normalize outputs)
5. `feat/data-balance-100k`
6. `feat/ml-threshold-runtime`
7. `feat/ui-threshold-indicator`
8. `feat/history-filters-feedback`

Ping me with:

* The **normalize** summary lines (per source) for your sample or full run, and
* The **dedupe** totals,

‚Ä¶and I‚Äôll hand you the `balance_split.py` scaffold and exact commands to land it.
