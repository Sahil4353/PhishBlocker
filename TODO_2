Perfect‚Äîhere‚Äôs a single, clean, **updated TODO / roadmap** that keeps everything you‚Äôve already written, plus what we actually finished, and exactly what we still need to do next (with the Git flow you asked for).

---

# PhishBlocker ‚Äî Status Snapshot (latest)

## What‚Äôs done ‚úÖ

### App / APIs / Infra

* FastAPI app with `create_app()` factory, async `lifespan`, and Request-ID middleware.
* Routers

  * **pages (HTML):** `/`, `POST /scan`, `/history`, `/scan/{id}/view` (end-to-end form ‚Üí pipeline ‚Üí DB ‚Üí detail view).
  * **scans (JSON/exports):** `POST /api/scan`, `GET /api/scans`, `GET /api/scan/{id}`, `GET /export/csv` (Excel formula hardening).
  * **debug:** `/health`, `/_debug/scan_count`, `/_debug/cwd`.
* Central logging & config: env-driven log level, Request-ID injected into all logs (also `uvicorn.*`), structured startup logs.
* App config (`app/core/config.py`): `ENV`, logging toggles, `MODEL_PATH`, `MODEL_VERSION`, `MODEL_METRICS_PATH`.
* Runtime model loading: `ModelService` instantiated once; graceful fallback to heuristics if artifact missing; attached to `app.state.model`.
* Database: SQLAlchemy `Email` & `Scan` models (FKs, indexes). `Scan` persists `label`, `confidence`, human-readable `reasons`, structured `details` (tokens/weights), full `probs` map, `header_flags`, `model_version`, `created_at`.
* Dev DB bootstraps via `Base.metadata.create_all()`; prod path assumes Alembic.
* CSV export with label/date filters and safe output.

### Services

* `parser.py`: robust RFC-822 parsing (headers, bodies, URLs, auth results, IPs, tolerant HTML‚Üítext).
* `heuristics.py`: lightweight text flags; `(label, confidence, reasons)` fallback.
* `scan_pipeline.py`: persists `Email`, runs heuristics and/or ML, writes full `Scan` row including `details` and `probs`.

### Model service

* `app/services/model.py`: loads joblib pipeline, exposes:

  * `predict_with_explanations(text, meta) -> (label, prob, reasons[])`
  * `predict_proba_map(text) -> {class: prob, ...}`
  * top-token explanations from LR coefficients; `.version` for audit.

### Trainer & tooling

* `scripts/train_baseline.py` (GPU/CPU-friendly TF-IDF + Torch LR):

  * resilient IO, label normalization, metrics JSON, PR/ROC and CM plots, optional temperature scaling, threshold tuning helpers.
* Predictor script exists; artifacts and datasets are **gitignored** and documented.

### Local smoke

* App runs on Windows with `uvicorn` (8080); `/`, `/history`, `/api/*` verified.
* Branch hygiene in place; you‚Äôve been committing small steps.

---

## In progress üöß

* **Threshold audit & persistence in pipeline** (branch `feat/ml-threshold-audit`)‚Äîinitial wiring committed.
* **Dataset acquisition**: Nazario download working; SpamAssassin cmd fixed (new URL set); Enron download plan finalized.

---

## Next up (M2 ‚Äî Data & thresholds) üéØ

Below are bite-sized tasks, each with a **branch name** and **git commands** you can run. I‚Äôve also listed which files I‚Äôll need from you (when relevant) so we can review code after you push.

### 1) Project layout refactor for scripts

**Goal:** split huge trainer into modules; add a clean ETL area for dataset transforms.

* **Branch:** `feat/scripts-refactor`
* **What to do**

  * Create folders:

    ```
    scripts/
      datasets/        # converters, cleaners, dedupe, balance
      train/           # training entrypoints
      utils/           # common IO, logging, text cleaning
    ```
  * Move `scripts/train_baseline.py` ‚Üí `scripts/train/train_baseline.py`
  * Add stubs:

    * `scripts/utils/io.py` (safe CSV/Parquet reader/writer)
    * `scripts/utils/text.py` (normalize whitespace, strip quotes/sigs placeholders)
    * `scripts/datasets/normalize_eml.py` (RFC822/mbox ‚Üí rows)
    * `scripts/datasets/dedupe_texts.py` (hash + near-dup hooks)
    * `scripts/datasets/balance_split.py` (class balancing, train/val split)
* **Git**

  ```
  git checkout -b feat/scripts-refactor
  git mv scripts/train_baseline.py scripts/train/train_baseline.py
  git add scripts/utils/io.py scripts/utils/text.py scripts/datasets/normalize_eml.py scripts/datasets/dedupe_texts.py scripts/datasets/balance_split.py
  git commit -m "refactor(scripts): split trainer and scaffold ETL modules (io, text, normalize_eml, dedupe, balance)"
  git push -u origin feat/scripts-refactor
  ```
* **Files I‚Äôll review after you push:** the new module stubs and the updated trainer import paths.

---

### 2) Dataset downloads (repeatable, scripted)

**Goal:** PowerShell scripts you can re-run to fetch/refresh datasets.

* **Branch:** `feat/data-downloaders`
* **What to do**

  * Add PS scripts:

    * `scripts/datasets/fetch_spamassassin.ps1`
    * `scripts/datasets/fetch_nazario.ps1`
    * `scripts/datasets/fetch_enron.ps1`
  * Each creates `data/raw/<source>` and downloads/extracts defensively.
* **Git**

  ```
  git checkout -b feat/data-downloaders
  git add scripts/datasets/fetch_spamassassin.ps1 scripts/datasets/fetch_nazario.ps1 scripts/datasets/fetch_enron.ps1
  git commit -m "feat(datasets): PowerShell downloaders for SpamAssassin, Nazario, and Enron (idempotent)"
  git push -u origin feat/data-downloaders
  ```
* **Files I‚Äôll review:** those `.ps1` scripts.

---

### 3) Normalization: EML/MBOX ‚Üí canonical CSV

**Goal:** produce `data/processed/<source>.csv` with **two columns**: `body_text`, `label` (one of `safe|spam|phishing`).

* **Branch:** `feat/data-normalize`
* **What to do**

  * Implement `scripts/datasets/normalize_eml.py`:

    * Walk:

      * SpamAssassin dirs (`easy_ham*`‚Üí`safe`, `spam*`‚Üí`spam`)
      * Nazario files (`phishing`)
      * Enron maildir (`safe`)
    * Use **your existing** `app/services/parser.py` logic (you can import it) to extract `body_text`.
    * Drop empties and extremely short texts (e.g., `< 20 chars`).
    * Save per-source CSVs to `data/processed/`.
* **Git**

  ```
  git checkout -b feat/data-normalize
  git add scripts/datasets/normalize_eml.py
  git commit -m "feat(datasets): normalize EML/maildir to canonical CSV {body_text,label} per source"
  git push -u origin feat/data-normalize
  ```
* **Files I‚Äôll review:** the `normalize_eml.py` implementation.

---

### 4) De-duplication (exact + near-dup hooks)

**Goal:** remove exact dupes and prep for near-dup removal without heavy deps (for now).

* **Branch:** `feat/data-dedupe`
* **What to do**

  * Implement `scripts/datasets/dedupe_texts.py`:

    * Exact dupes: SHA256 on normalized text (lowercase, collapse whitespace).
    * Keep first occurrence per (text,label); log counts removed.
    * Leave hooks (functions) for optional near-dup (e.g., MinHash/SimHash) later.
* **Git**

  ```
  git checkout -b feat/data-dedupe
  git add scripts/datasets/dedupe_texts.py
  git commit -m "feat(datasets): exact dedupe by text hash; hooks for near-dup"
  git push -u origin feat/data-dedupe
  ```
* **Files I‚Äôll review:** the dedupe script.

---

### 5) Balancing to target counts (‚â•100k per class)

**Goal:** produce a single balanced dataset with **100k per class** minimum (`safe`, `spam`, `phishing`).

* **Branch:** `feat/data-balance-100k`
* **What to do**

  * Implement `scripts/datasets/balance_split.py`:

    * Load normalized + deduped CSVs.
    * Compute available counts per class.
    * Strategy:

      * **safe:** downsample from Enron to target.
      * **spam:** combine SpamAssassin (all spam sets) + any other spam we add later; if short, reuse with **different seeds** (augment by light transforms only if needed‚Äîe.g., whitespace jitter‚Äîotherwise document shortfall).
      * **phishing:** Nazario is small; we‚Äôll need to add more phishing sources (coming in M2.1). For now, parameterize `--target-per-class 100000` and **warn** if not met; still produce the max feasible balanced set.
    * Output:

      * `data/processed/mix_balanced.csv` and `mix_balanced.parquet`
      * deterministic `train/val` split (e.g., 80/20).
* **Git**

  ```
  git checkout -b feat/data-balance-100k
  git add scripts/datasets/balance_split.py
  git commit -m "feat(datasets): balance + split pipeline targeting 100k/class; warns when sources underfill"
  git push -u origin feat/data-balance-100k
  ```
* **Files I‚Äôll review:** the balancing script.

---

### 6) Metrics & thresholds loader at runtime

**Goal:** load `*.metrics.json` and apply per-class thresholds in inference.

* **Branch:** `feat/ml-threshold-runtime`
* **What to do**

  * Add `app/services/thresholds.py`:

    * Read `settings.MODEL_METRICS_PATH`.
    * Provide `get_thresholds()` and `decide_label(probs, thresholds)` (e.g., `if probs['phishing']‚â•œÑ_p -> 'phishing'; elif probs['spam']‚â•œÑ_s -> 'spam'; else 'safe'`).
  * Update `scan_pipeline.py` to:

    * Use **threshold decision** when `model` exists (store the chosen œÑ values in `Scan.details` under `thresholds_used`).
* **Git**

  ```
  git checkout -b feat/ml-threshold-runtime
  git add app/services/thresholds.py
  git commit -m "feat(ml): load thresholds from metrics.json and apply decision policy in pipeline"
  git add app/services/scan_pipeline.py
  git commit -m "chore(pipeline): persist thresholds_used and final_decision_source"
  git push -u origin feat/ml-threshold-runtime
  ```
* **Files I‚Äôll review:** `thresholds.py`, pipeline changes.

---

### 7) UI/UX: surface operating point

**Goal:** show users the current threshold policy and confidence.

* **Branch:** `feat/ui-threshold-indicator`
* **What to do**

  * On `/scan/{id}/view`, display:

    * Final label & confidence,
    * Threshold used for that class (e.g., ‚ÄúœÑ_phishing = 0.72‚Äù),
    * Top tokens (already available in `Scan.details`).
* **Git**

  ```
  git checkout -b feat/ui-threshold-indicator
  # update template(s): app/api/routes/pages.py + Jinja templates
  git add <updated templates and route>
  git commit -m "feat(ui): show threshold policy & top tokens on scan detail"
  git push -u origin feat/ui-threshold-indicator
  ```
* **Files I‚Äôll review:** updated page template/route.

---

### 8) Filtering & feedback (kickoff)

**Goal:** basic filters + feedback capture for future retraining.

* **Branch:** `feat/history-filters-feedback`
* **What to do**

  * Add query params to `/history` and `/api/scans` (label/sender/date).
  * On detail page, add simple ‚ÄúCorrect label to: ‚Ä¶‚Äù (stores a correction row; schema TBD).
* **Git**

  ```
  git checkout -b feat/history-filters-feedback
  git add app/api/routes/scans.py app/api/routes/pages.py app/models/<corrections>.py
  git commit -m "feat(history): filters for label/sender/date and basic feedback capture"
  git push -u origin feat/history-filters-feedback
  ```
* **Files I‚Äôll review:** route changes and minimal model for feedback.

---

## Stretch (soon after M2)

* CI: pre-commit (black/ruff/isort), pytest seeds (parser & API), GitHub Actions (lint+test), Dockerfile + Alembic on start.
* Data retention: TTL job to redact `Email.body_text/html_raw` after N days; keep `Scan` metadata.
* Near-duplicate removal: MinHash/SimHash optional module when we scale up phishing sources.

---

## Running order (recommended)

1. `feat/scripts-refactor` ‚Üí 2) `feat/data-downloaders` ‚Üí 3) `feat/data-normalize` ‚Üí 4) `feat/data-dedupe` ‚Üí 5) `feat/data-balance-100k` ‚Üí 6) `feat/ml-threshold-runtime` ‚Üí 7) `feat/ui-threshold-indicator` ‚Üí 8) `feat/history-filters-feedback`.

I‚Äôll review each branch as you push the files I listed.
When you‚Äôre ready to start the **downloads**, run the PowerShell scripts we prepared, then we‚Äôll wire up **normalize ‚Üí dedupe ‚Üí balance** and move on to thresholds.
