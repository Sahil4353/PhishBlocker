awesome‚Äîhere‚Äôs your **updated status snapshot** with the ML work you just landed, plus a clear view of branches and some practical git hygiene for next steps.

# PhishBlocker ‚Äî Status Snapshot (updated)

## What‚Äôs done ‚úÖ

### App / APIs / Infra (unchanged from last snapshot)

* **FastAPI app** with app-factory + lifespan and request-ID middleware.
* **Routers**

  * **pages (HTML):** `/`, `POST /scan`, `/history`, `/scan/{id}/view`
  * **scans (JSON/exports):** `GET /scan/{id}`, `GET /export/csv`
  * **debug:** `/health`, `/_debug/scan_count`, `/_debug/cwd`
* **Logging & config:** env-driven logging; request-ID on all log lines; `uvicorn.*` filters.
* **Database & migrations:** `Email` + `Scan` with FK and indexes; SQLite tuned (FK ON, WAL, etc.); Alembic wired.
* **Batch ingestion:** `scripts/ingest_enron.py` ‚Üí `data/processed/enron_parsed.csv`.
* **Services (present, unchanged):** `parser.py`, `heuristics.py`, `scan_pipeline.py`.
* **Pages/UI:** history, detail, CSV export (safe vs Excel formulas).
* **DB access:** shared `get_db()` in `app/db.py`.

### New: ML baseline & data prep

* **Training script**: `scripts/train_baseline.py`

  * TF-IDF **word(1‚Äì2)** + **char(3‚Äì5)**, `min_df=3`, English stop-words, `float32`, feature caps.
  * **Logistic Regression (default):** SAGA, elastic-net (`l1_ratio=0.05`), `C=0.5`, `max_iter=5000`, `class_weight="balanced"`, seeded.
  * **Optional `--ensemble`** (not used in app yet): LR + MultinomialNB + calibrated LinearSVM (Platt/sigmoid, `cv=3`) with soft-voting.
  * Safe probability handling, confusion matrix & per-class PR plots, rich `*.metrics.json`.
* **Smoke predictor**: `scripts/predict.py` (prints label, per-class probs, and LR token ‚Äúreasons‚Äù).
* **Public datasets (processed in-repo)**

  * `scripts/prepare_spamassassin.py` ‚Üí `data/processed/spamassassin_parsed.csv` (**4,669** rows).
  * `scripts/prepare_nazario.py` ‚Üí `data/processed/nazario_phishing.csv` (**1,565** rows).
  * (You also trained once on a **mix**; see results below.)
* **Artifacts (latest LR run)**

  * Trained on: `enron_parsed.csv` + SpamAssassin + Nazario (or the smaller *mix* run).
  * Example artifacts:

    * `models/tfidf_lr_fast.joblib`
    * `models/tfidf_lr_fast.metrics.json`
    * `models/tfidf_lr_fast.cm.png`, `models/tfidf_lr_fast.pr.phishing.png`
* **Results (on the \~156k ‚Äúmix‚Äù run)**

  * Classes: `['phishing', 'safe', 'spam']`
  * Macro-F1 ‚âà **0.723**

    * phishing: P=0.487, **R=0.994**, F1=0.653
    * safe: **P=1.000**, R=0.968, F1=0.984
    * spam: P=0.363, **R=0.995**, F1=0.532
  * Takeaway: current LR is **recall-oriented** on minority classes (great for catching bad mail), but **precision is low** for phishing/spam on this split. That‚Äôs expected with heavy class imbalance; we‚Äôll tune thresholds and data balance next.

> ‚ö†Ô∏è **Services**: unchanged on purpose. Keep using the LR artifact with the CLI for now. Don‚Äôt plug an ensemble artifact into the current service until we add the tiny ‚Äúuse LR‚Äôs coefficients for reasons even when ensemble predicts‚Äù shim.

---

## Branches & commits

### Current branch structure

* **develop** ‚Äî upstream base (not the branch you‚Äôre working from).
* **feat/services-pipeline** ‚Äî app services (parser/heuristics/pipeline).
* **feat/ml/baseline-v2** ‚Äî *your active ML branch* (branched from `feat/services-pipeline`, tracking `origin/feat/ml/baseline-v2`).
* **feat/data-enron-ingest** ‚Äî ingestion + schema foundations.

### Suggested day-to-day flows

* Keep ML isolated:

  ```powershell
  # keep your branch current with services-pipeline (no merge commits)
  git fetch origin
  git rebase origin/feat/services-pipeline
  # resolve, test, then push
  git push --force-with-lease
  ```
* When services work is merged to `develop`, **rebase ML onto `develop`** once, then open a PR from `feat/ml/baseline-v2` ‚Üí `develop`.

---

## What changed since the last snapshot üîÑ

* Added full **ML baseline** (trainer, predictor, metrics & plots, thresholds in JSON).
* Added **public data prep** scripts + processed corpora.
* Trained first **LR model** with measurable metrics; CLI **reasons** working.
* Implemented optional **ensemble flag** (kept off in app; CLI only).
* No changes to `app/services` yet (by design).

---

## Smoke checklist (now)

* Train (fast footprint)

  ```powershell
  python scripts\train_baseline.py `
    --inputs data\processed\mix_balanced.csv `
    --val-size 0.2 --seed 42 `
    --max-features-word 80000 --max-features-char 80000 `
    --out models\tfidf_lr_fast.joblib
  ```
* Predict (CLI)

  ```powershell
  python scripts\predict.py --model models\tfidf_lr_fast.joblib `
    "verify your account urgently to continue" `
    "free gift voucher click here" `
    "team lunch next Thursday at 12"
  ```
* Inspect metrics & threshold suggestions

  ```powershell
  python - << 'PY'
  ```

import json, pprint
m="models/tfidf\_lr\_fast.metrics.json"
pp=json.load(open(m,encoding="utf-8"))
print("Macro-F1:", round(pp\["metrics"]\["macro avg"]\["f1-score"],3))
print("Threshold suggestions:"); pprint.pp(pp.get("threshold\_suggestions",{}))
PY

```

---

## Roadmap (updated)

### M1 ‚Äî MVP polish (Local) ¬∑ **in progress**
- [ ] Wire `pages.py` `POST /scan` to `create_scan_from_text(...)` (persist Email+Scan consistently).
- [ ] Add `/api/scan (POST)` + `/api/scans (GET)`.
- [ ] Use **LR artifact** only (no ensemble) in the first wiring.

### M2 ‚Äî Data & thresholds ¬∑ **next**
- [ ] Increase minority examples (add modern phishing corpora; de-dup; basic cleaning).
- [ ] Downsample safe or up-weight hard negatives; evaluate.
- [ ] Adopt **threshold_suggestions** from metrics JSON for UI defaults (e.g., quarantine if P(phishing) ‚â• T\_98P).

### M3 ‚Äî Services integration (ML) ¬∑ **after M1/M2**
- [ ] Add `app/services/model.py` (or extend existing) to load joblib once and expose:
`predict_with_explanations(text) -> (label, prob, reasons)`.
- [ ] If classifier is an ensemble, pull LR sub-estimator coefficients for ‚Äúreasons‚Äù.
- [ ] Make thresholds configurable via env or DB.

### M4 ‚Äî Feedback & filters
- [ ] Feedback table + ‚ÄúCorrect label‚Äù UI.
- [ ] History filters: label/date/sender search.

### M5 ‚Äî CI & packaging
- [ ] Pre-commit, pytest, GitHub Actions (lint/test/build).
- [ ] Docker image; Alembic on startup.

### M6 ‚Äî Safety & retention
- [ ] TTL for raw bodies; purge jobs; security headers; rate limiting.

---

## Git recommendations (future-proofing)

**Branching**
- Keep features on short-lived branches: `feat/<area>-<slug>`.
- Rebase feature branches onto the target (no long-running diverged histories).
- Prefer **squash & merge** for clean history.

**PRs**
- Small, single-purpose PRs (‚â§ 400 lines diff ideally).
- Template with: What/Why/How/Test plan/Risks.
- CODEOWNERS for app vs. ML reviewers.

**Versioning & artifacts**
- Don‚Äôt commit data or model binaries. Keep `data/**` and `models/**` in `.gitignore`.
- Tag code-only releases, not model files:  
`git tag ml/v2-<yyyy-mm-dd>` on commit that produced the artifact.
- Store artifacts in a bucket or release storage, and record the path/URI in `*.metrics.json` (we already save metadata; we can add an `artifact_uri` later).

**.gitignore / large files**
- Ensure `.db`, `*.joblib`, `*.png`, `*.json` under `models/`, and `data/raw|processed/` are ignored (looks good already).

**Quality gates**
- Add `pre-commit` (black, ruff, isort), basic `pytest -q`, and mypy (optional) before merging to `develop`.

---

## Immediate next steps (pick one)

**A) Wire LR into the app (small PR)**
- Add `app/services/model.py` (loader + `predict_with_explanations` using LR).
- In `scan_pipeline.py`, optionally call the model; store `label`, `confidence`, and ‚Äúreasons‚Äù.
- Update `POST /scan` to use pipeline.

**B) Improve data balance**
- Create a balanced training set (downsample safe, or stratified cap per class), retrain LR, compare Macro-F1 & phishing precision.

**C) Evaluate ensemble (offline)**
- Train with `--ensemble`, compare phishing PR-AUC & Macro-F1. If it wins without latency pain, we‚Äôll add the explanation shim and consider promoting it.

Tell me which track you want to execute right now, and I‚Äôll drop the exact diff/patch (tiny, focused) to get it done.
::contentReference[oaicite:0]{index=0}
```
