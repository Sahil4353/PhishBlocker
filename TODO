# PhishBlocker ‚Äî Status Snapshot (current code)

## What‚Äôs done ‚úÖ

* **FastAPI app** with clean app factory + lifespan and request-ID middleware.

  * Routers:

    * **pages** (HTML): `/`, `POST /scan`, `/history`, `/scan/{id}/view`
    * **scans** (JSON/exports): `GET /scan/{id}`, `GET /export/csv`
    * **debug**: `/health`, `/_debug/scan_count`, `/_debug/cwd`
* **Logging & config**

  * Centralized env-driven logging (`ENABLE_LOGGING`, `LOG_LEVEL`, `ENV`).
  * Request ID injected into every log line; filters attached to `uvicorn.*`.
* **Database & migrations**

  * Models: **Email** (normalized message) + **Scan** (classification), FK `scans.email_id ‚Üí emails.id (ON DELETE CASCADE)`.
  * Indexes on sender/timestamps/labels; check constraint on confidence.
  * Alembic wired to import `app.models` so autogenerate sees both tables.
  * SQLite tuned (FKs ON, WAL, synchronous=NORMAL, busy\_timeout=30s).
* **Data ingestion (batch)**

  * `scripts/ingest_enron.py` parses raw Enron CSV ‚Üí **data/processed/enron\_parsed.csv** (verified).
* **Services (new)**

  * `services/parser.py`: RFC-822 `.eml` ‚Üí provider-agnostic dict (headers, bodies, URLs, auth results, received IPs).
  * `services/heuristics.py`: keeps `classify_text(text)` + adds `analyze(parsed)` flags.
  * `services/scan_pipeline.py`: one-stop **parse ‚Üí analyze ‚Üí persist Email+Scan** (optional ML model hook).
* **Pages/UI**

  * History page with pagination; detail page for a scan; CSV export (label/date filters).
  * CSV export guarded against Excel formula injection.
* **DB access**

  * Shared `get_db()` dependency in `app/db.py`; routes can import directly.

## Branches & commits

* `feat/data-enron-ingest` ‚Äî ingestion + schema + logging + routes (foundation).
* `feat/services-pipeline` ‚Äî **added** `parser.py` & `scan_pipeline.py` (tracking `origin/feat/services-pipeline`), no new migrations.

---

## What changed since the last snapshot üîÑ

* **Added** `services/parser.py` and `services/scan_pipeline.py`.
* **Kept** existing HTML flow (`POST /scan` in `pages.py`) using `classify_text` ‚Äî pipeline is ready but not yet wired into routes.
* **Improved** scans API CSV export and docs grouping (tags).
* **Shared** `get_db()` moved to `app/db.py` (routes can be DRY‚Äôd to use it).

---

## Smoke checklist (now)

* Run app:

  ```
  uvicorn app.main:app --reload
  ```
* Hit:

  * `/` (form), `/history`, `/scan/{id}/view`
  * `/docs` & `/openapi.json`
  * `/export/csv?label=safe&date_from=2025-01-01`
  * `/health`, `/_debug/scan_count`
* Quick pipeline demo (optional REPL):

  ```py
  from sqlalchemy.orm import Session
  from sqlalchemy import create_engine
  from app.services.scan_pipeline import create_scan_from_text
  from app.models import Scan

  e = create_engine("sqlite:///phishblocker.db", future=True)
  from sqlalchemy.orm import Session
  with Session(e) as s:
      sc = create_scan_from_text(s, "Please verify your account at http://1.2.3.4/login", subject="Action required", sender="it@corp.com")
      print(sc.id, sc.label, sc.confidence, sc.email_id)
  ```

---

## Roadmap (updated)

### M1 ‚Äî MVP polish (Local) ¬∑ in progress

* [ ] Switch `pages.py` **POST /scan** to use `create_scan_from_text` (persists Email + Scan consistently).
* [ ] Add **/api/scan (POST)** and **/api/scans (GET)** endpoints (JSON clients).
* [ ] Basic tests:

  * Unit: `classify_text()` & `analyze(parsed)`
  * Integration: `POST /scan` persists ‚Üî `GET /scan/{id}` returns; CSV export filters.

### M2 ‚Äî Gmail ingestion

* OAuth + token capture ‚Üí fetch `.eml` ‚Üí feed **create\_scan\_from\_eml\_bytes** ‚Üí show in History.
* Optional: webhook (`/webhooks/gmail`) later; start with pull.

### M3 ‚Äî ML baseline

* TF-IDF + Logistic Regression (+ calibration); joblib with version string.
* Integrate via `model.predict_with_explanations(text, meta)` in pipeline.

### M4 ‚Äî Feedback & filters

* Detail page: ‚ÄúCorrect label‚Äù (Safe/Spam/Phish) + notes ‚Üí table `feedback` for retraining.
* History filters: label/date/sender text search.

### M5 ‚Äî Packaging & CI

* Docker + Alembic on startup; GitHub Actions (lint/test/build).
* Structured logs; basic metrics.

### M6 ‚Äî Safety & retention

* TTL for raw bodies; purge job; security headers; rate limiting.

---

## Risks & guardrails

* **Dataset drift**: include modern ham + adversarial phish (punycode/IP/shorteners).
* **Webhook idempotency (future)**: dedupe by Gmail `messageId`/`historyId`.
* **Explainability**: reasons from top features + heuristics flags.
* **SQLite locking**: mitigated with WAL + busy\_timeout; can move to Postgres when deploying.

---

## Immediate next steps (pick one)

**A) Wire the pipeline (quick win)**

* Change `pages.py` `POST /scan` to call `create_scan_from_text(...)`.
* Add `POST /api/scan` (accepts `body_text` or raw `.eml` upload) using pipeline.
* Return `{scan_id, email_id, label, confidence, reasons, header_flags}`.

**B) Tests & CI**

* Add 3‚Äì5 unit tests + 1 integration, wire `pytest` in GH Actions.

**C) Model baseline**

* Start a simple TF-IDF LR; store `model_version`, return top reasons.

Tell me which track you want next and I‚Äôll drop in the exact code changes (no churn, minimal diff).
